{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A jupyter notebook version file for the `main.py`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `autoreload` to execute the change in `.py` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = \"D:/Databases/CAS(ME)^2\"\n",
    "# dataset_dir = \"I:/HEH/Databases/CAS(ME)^2\"\n",
    "# dataset_dir = \"/data/disk1/heh/databases/CAS(ME)^2\"\n",
    "\n",
    "test_dataset_dir = \"D:/Databases/MEGC2022_testSet/CAS_Test_cropped\"\n",
    "# test_dataset_dir = \"I:/HEH/Databases/MEGC2022_testSet/CAS_Test_cropped\"\n",
    "# test_dataset_dir = \"/data/disk1/heh/databases/MEGC2022_testSet/CAS_Test_cropped\"\n",
    "\n",
    "images_loading = False\n",
    "image_size = 128\n",
    "load_cropped_images = False\n",
    "expression_type = \"micro-expression\"\n",
    "# expression_type = \"macro-expression\"\n",
    "save_x = False\n",
    "debug_preds = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing import *\n",
    "\n",
    "# crop_images_dev(test_dataset_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debug the image processing, the videos_images is from cropped_rawpic, whereas the other variables are from rawpic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  s15\n",
      "subject:  s16\n",
      "subject:  s19\n",
      "subject:  s20\n",
      "subject:  s21\n",
      "subject:  s22\n",
      "subject:  s23\n",
      "subject:  s24\n",
      "subject:  s25\n",
      "subject:  s26\n",
      "subject:  s27\n",
      "subject:  s29\n",
      "subject:  s30\n",
      "subject:  s31\n",
      "subject:  s32\n",
      "subject:  s33\n",
      "subject:  s34\n",
      "subject:  s35\n",
      "subject:  s36\n",
      "subject:  s37\n",
      "subject:  s38\n",
      "subject:  s40\n"
     ]
    }
   ],
   "source": [
    "from image_processing import *\n",
    "\n",
    "# videos_images, subjects, subjects_videos_code = load_images(dataset_dir)\n",
    "\n",
    "videos_images, subjects, subjects_videos_code = load_images_dev(\n",
    "    dataset_dir,\n",
    "    images_loading=images_loading,\n",
    "    image_size=image_size,\n",
    "    load_cropped_images=load_cropped_images,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: ['s15', 's16', 's19', 's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's29', 's30', 's31', 's32', 's33', 's34', 's35', 's36', 's37', 's38', 's40']\n",
      "subjects_videos_code: [['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0508'], ['0101', '0102', '0401', '0402', '0502', '0505', '0507'], ['0102', '0402', '0505', '0507', '0502'], ['0502'], ['0101', '0401'], ['0101', '0102', '0402', '0503', '0508'], ['0102', '0402', '0503', '0507'], ['0101', '0401', '0402', '0502', '0507'], ['0101', '0102', '0502', '0508'], ['0101', '0102', '0401', '0503'], ['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0507', '0508'], ['0502'], ['0101', '0102', '0401', '0502', '0503', '0505', '0507'], ['0101', '0401', '0402', '0502', '0503', '0505', '0507'], ['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0507', '0508'], ['0102', '0402'], ['0401', '0402', '0503'], ['0102'], ['0401', '0505'], ['0101', '0402', '0502', '0505', '0507', '0508'], ['0502', '0507'], ['0401', '0502', '0503']]\n"
     ]
    }
   ],
   "source": [
    "print(\"subjects:\", subjects)\n",
    "print(\"subjects_videos_code:\", subjects_videos_code)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  sub01\n",
      "subject:  sub02\n",
      "subject:  sub03\n",
      "subject:  sub04\n",
      "subject:  sub05\n"
     ]
    }
   ],
   "source": [
    "from image_processing import *\n",
    "\n",
    "test_videos_images, test_subjects, test_videos_name = load_images_dev(\n",
    "    test_dataset_dir, images_loading=images_loading, image_size=image_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_subjects: ['sub01', 'sub02', 'sub03', 'sub04', 'sub05']\n",
      "test_videos_name: ['sub01', 'sub02', 'sub03', 'sub04', 'sub05']\n"
     ]
    }
   ],
   "source": [
    "print(\"test_subjects:\", test_subjects)\n",
    "print(\"test_videos_name:\", test_videos_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Excel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_processing import load_excel\n",
    "\n",
    "Excel_data = load_excel(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>video_name_&amp;_expression_number</th>\n",
       "      <th>onset</th>\n",
       "      <th>apex</th>\n",
       "      <th>offset</th>\n",
       "      <th>AUs</th>\n",
       "      <th>extimated_emotion</th>\n",
       "      <th>expression_type</th>\n",
       "      <th>self-reported_emotion</th>\n",
       "      <th>video_name</th>\n",
       "      <th>video_code</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>anger1_1</td>\n",
       "      <td>557</td>\n",
       "      <td>572</td>\n",
       "      <td>608</td>\n",
       "      <td>4+10+14+15</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger1</td>\n",
       "      <td>0401</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anger1_2</td>\n",
       "      <td>2854</td>\n",
       "      <td>2862</td>\n",
       "      <td>2871</td>\n",
       "      <td>38</td>\n",
       "      <td>others</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>anger1</td>\n",
       "      <td>0401</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_1</td>\n",
       "      <td>2155</td>\n",
       "      <td>2163</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_2</td>\n",
       "      <td>3363</td>\n",
       "      <td>3371</td>\n",
       "      <td>3383</td>\n",
       "      <td>4+7+14</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_3</td>\n",
       "      <td>3380</td>\n",
       "      <td>3386</td>\n",
       "      <td>3407</td>\n",
       "      <td>4+14+38</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant video_name_&_expression_number  onset  apex  offset  \\\n",
       "0            1                       anger1_1    557   572     608   \n",
       "1            1                       anger1_2   2854  2862    2871   \n",
       "2            1                       anger2_1   2155  2163       0   \n",
       "3            1                       anger2_2   3363  3371    3383   \n",
       "4            1                       anger2_3   3380  3386    3407   \n",
       "\n",
       "          AUs extimated_emotion   expression_type self-reported_emotion  \\\n",
       "0  4+10+14+15          negative  macro-expression                 anger   \n",
       "1          38            others  macro-expression               sadness   \n",
       "2         NaN          negative  macro-expression                 anger   \n",
       "3      4+7+14          negative  macro-expression                 anger   \n",
       "4     4+14+38          negative  macro-expression                 anger   \n",
       "\n",
       "  video_name video_code subject  \n",
       "0     anger1       0401     s15  \n",
       "1     anger1       0401     s15  \n",
       "2     anger2       0402     s15  \n",
       "3     anger2       0402     s15  \n",
       "4     anger2       0402     s15  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Excel_data.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_videos_index:  [1, 4, 8, 9, 12, 13, 14, 16, 28, 33, 36, 37, 38, 45, 46, 47, 49, 50, 52, 54, 55, 57, 62, 64, 67, 71, 73, 74, 77, 83, 87, 91, 93]\n",
      "len(clean_videos_images) = 33\n"
     ]
    }
   ],
   "source": [
    "from label_processing import load_ground_truth_labels\n",
    "\n",
    "\n",
    "(\n",
    "    clean_videos_images,\n",
    "    clean_subjects_videos_code,\n",
    "    clean_subjects,\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    ") = load_ground_truth_labels(\n",
    "    dataset_dir,\n",
    "    expression_type,\n",
    "    videos_images,\n",
    "    subjects_videos_code,\n",
    "    subjects,\n",
    "    Excel_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(clean_subjects):  14\n",
      "clean_subjects:  ['s15' 's16' 's19' 's23' 's24' 's25' 's27' 's29' 's30' 's31' 's32' 's35'\n",
      " 's37' 's38']\n",
      "len(clean_subjects_videos_code):  14\n",
      "clean_subjects_videos_codes:  [['0102', '0502'], ['0101', '0102', '0502', '0505', '0507'], ['0402'], ['0102'], ['0401', '0507'], ['0101', '0102'], ['0101', '0102', '0401', '0502', '0503', '0507'], ['0502'], ['0101', '0401'], ['0101', '0402', '0505'], ['0401', '0502', '0503', '0508'], ['0102'], ['0402', '0508'], ['0507']]\n",
      "len(clean_subjects_videos_ground_truth_labels):  14\n",
      "clean_subjects_videos_ground_truth_labels[6]:  [[[873, 887]], [[33, 47], [308, 316], [373, 387]], [[351, 364], [368, 381], [1134, 1146], [1973, 1985]], [[612, 627]], [[418, 431]], [[875, 889]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"len(clean_subjects): \", len(clean_subjects))\n",
    "print(\"clean_subjects: \", clean_subjects)\n",
    "print(\"len(clean_subjects_videos_code): \", len(clean_subjects_videos_code))\n",
    "print(\"clean_subjects_videos_codes: \", clean_subjects_videos_code)\n",
    "print(\n",
    "    \"len(clean_subjects_videos_ground_truth_labels): \",\n",
    "    len(clean_subjects_videos_ground_truth_labels),\n",
    ")\n",
    "\n",
    "# 7 (s23) has happy1 (0502) in excel but the folder name is happy2 (0503)\n",
    "print(\n",
    "    \"clean_subjects_videos_ground_truth_labels[6]: \",\n",
    "    clean_subjects_videos_ground_truth_labels[6],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 s15: ['0102', '0502'], ground truth len: 2\n",
      "1 s16: ['0101', '0102', '0502', '0505', '0507'], ground truth len: 8\n",
      "2 s19: ['0402'], ground truth len: 1\n",
      "3 s23: ['0102'], ground truth len: 3\n",
      "4 s24: ['0401', '0507'], ground truth len: 8\n",
      "5 s25: ['0101', '0102'], ground truth len: 4\n",
      "6 s27: ['0101', '0102', '0401', '0502', '0503', '0507'], ground truth len: 11\n",
      "7 s29: ['0502'], ground truth len: 1\n",
      "8 s30: ['0101', '0401'], ground truth len: 2\n",
      "9 s31: ['0101', '0402', '0505'], ground truth len: 6\n",
      "10 s32: ['0401', '0502', '0503', '0508'], ground truth len: 6\n",
      "11 s35: ['0102'], ground truth len: 2\n",
      "12 s37: ['0402', '0508'], ground truth len: 2\n",
      "13 s38: ['0507'], ground truth len: 1\n",
      "total len:  57\n"
     ]
    }
   ],
   "source": [
    "total_len = 0\n",
    "for index, clean_subject_videos_code in enumerate(clean_subjects_videos_code):\n",
    "    ground_truth_len = 0\n",
    "    for i in clean_subjects_videos_ground_truth_labels[index]:\n",
    "        for j in i:\n",
    "            ground_truth_len += 1\n",
    "    print(\n",
    "        f\"{index} {clean_subjects[index]}: {clean_subject_videos_code}, ground truth len: {ground_truth_len}\"\n",
    "    )\n",
    "    total_len += ground_truth_len\n",
    "print(\"total len: \", total_len)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k (Half of average length of expression) =  6\n"
     ]
    }
   ],
   "source": [
    "from label_processing import calculate_k\n",
    "\n",
    "k = calculate_k(clean_subjects_videos_ground_truth_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Pro-process\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 77 m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features from clean_video 0 image 1985\n",
      "Finish extract features form required_video_index  0\n",
      "pre-processing clean_video 0, image 1985,\n",
      "Finish pre-process clean_video_images_index  0\n",
      "extracting features from clean_video 1 image 3294\n",
      "Finish extract features form required_video_index  1\n",
      "pre-processing clean_video 1, image 3294,\n",
      "Finish pre-process clean_video_images_index  1\n",
      "extracting features from clean_video 2 image 1551\n",
      "Finish extract features form required_video_index  2\n",
      "pre-processing clean_video 2, image 1551,\n",
      "Finish pre-process clean_video_images_index  2\n",
      "extracting features from clean_video 3 image 3418\n",
      "Finish extract features form required_video_index  3\n",
      "pre-processing clean_video 3, image 3418,\n",
      "Finish pre-process clean_video_images_index  3\n",
      "extracting features from clean_video 4 image 2839image 989\n",
      "Finish extract features form required_video_index  4\n",
      "pre-processing clean_video 4, image 2839,\n",
      "Finish pre-process clean_video_images_index  4\n",
      "All features extracted and pre-processed.\n"
     ]
    }
   ],
   "source": [
    "from features_extraction_and_pre_processing import *\n",
    "\n",
    "# extract_features_and_pre_process_test(\n",
    "#     clean_videos_images=test_videos_images,\n",
    "#     k=k,\n",
    "#     expression_type=expression_type,\n",
    "#     test_dataset_dir=test_dataset_dir,\n",
    "#     image_size=image_size,\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 260 m.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Optical Flow Features (shape = [128, 128, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features_extraction import extract_features\n",
    "\n",
    "# clean_videos_images_features = extract_features(clean_videos_images, k, image_size=128)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 44 m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import pre_process\n",
    "\n",
    "# resampled_clean_videos_images_features = preprocess(\n",
    "#     clean_videos_images, clean_videos_images_features, k\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if expression_type == \"micro-expression\":\n",
    "#     with open(\n",
    "#         Path(\n",
    "#             dataset_dir,\n",
    "#             \"resampled_clean_videos_images_me_features_\" + str(image_size) + \".pkl\",\n",
    "#         ),\n",
    "#         \"wb\",\n",
    "#     ) as pkl_file:\n",
    "#         _pickle.dump(resampled_clean_videos_images_features, pkl_file)\n",
    "#         pkl_file.close()\n",
    "# elif expression_type == \"macro-expression\":\n",
    "#     with open(\n",
    "#         Path(\n",
    "#             dataset_dir,\n",
    "#             \"resampled_clean_videos_images_mae_features_\" + str(image_size) + \".pkl\",\n",
    "#         ),\n",
    "#         \"wb\",\n",
    "#     ) as pkl_file:\n",
    "#         _pickle.dump(resampled_clean_videos_images_features, pkl_file)\n",
    "#         pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load **Original** Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if expression_type == \"micro-expression\":\n",
    "#     with open(\n",
    "#         Path(\n",
    "#             dataset_dir,\n",
    "#             \"original_resampled_clean_videos_images_me_features.pkl\",\n",
    "#         ),\n",
    "#         \"rb\",\n",
    "#     ) as pkl_file:\n",
    "#         resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "#         pkl_file.close()\n",
    "# elif expression_type == \"macro-expression\":\n",
    "#     with open(\n",
    "#         Path(\n",
    "#             dataset_dir,\n",
    "#             \"original_resampled_clean_videos_images_mae_features.pkl\",\n",
    "#         ),\n",
    "#         \"rb\",\n",
    "#     ) as pkl_file:\n",
    "#         resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "#         pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_x is True or debug_preds is False:\n",
    "    if expression_type == \"micro-expression\":\n",
    "        with open(\n",
    "            Path(\n",
    "                dataset_dir,\n",
    "                \"resampled_clean_videos_images_me_features_\" + str(image_size) + \".pkl\",\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as pkl_file:\n",
    "            resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "            pkl_file.close()\n",
    "    elif expression_type == \"macro-expression\":\n",
    "        with open(\n",
    "            Path(\n",
    "                dataset_dir,\n",
    "                \"resampled_clean_videos_images_mae_features_\"\n",
    "                + str(image_size)\n",
    "                + \".pkl\",\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as pkl_file:\n",
    "            resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "            pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_x is True or debug_preds is False:\n",
    "    print(\n",
    "        \"len(resampled_clean_videos_images_features): \",\n",
    "        len(resampled_clean_videos_images_features),\n",
    "    )\n",
    "    print(\n",
    "        \"len(resampled_clean_videos_images_features[0]): \",\n",
    "        len(resampled_clean_videos_images_features[0]),\n",
    "    )\n",
    "    print(\n",
    "        \"resampled_clean_videos_images_features[0][0].shape: \",\n",
    "        resampled_clean_videos_images_features[0][0].shape,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 231999\n"
     ]
    }
   ],
   "source": [
    "from labeling import *\n",
    "\n",
    "labels = pseudo_labeling(\n",
    "    clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    ")\n",
    "\n",
    "# labels = original_labeling(\n",
    "#     clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from training_test_dev import train_and_test\n",
    "\n",
    "model_names = {\n",
    "    0: \"SOFTNet\",\n",
    "    1: \"SOFTNetCBAM\",\n",
    "    2: \"ViT\",\n",
    "    3: \"SL-ViT\",\n",
    "    4: \"Swin-T\",\n",
    "    5: \"Swin-S\",\n",
    "    6: \"SL-Swin-T\",\n",
    "    7: \"SL-Swin-S\",\n",
    "}\n",
    "\n",
    "\n",
    "if debug_preds is False:\n",
    "    preds = train_and_test(\n",
    "        dataset_dir,\n",
    "        test_dataset_dir,\n",
    "        clean_subjects,\n",
    "        test_videos_name,\n",
    "        y=labels,\n",
    "        expression_type=expression_type,\n",
    "        model_name=model_names[6],\n",
    "        train_or_not=True,\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "    )\n",
    "else:\n",
    "    with open(\n",
    "        Path(\n",
    "            dataset_dir,\n",
    "            \"preds\",\n",
    "            \"mae_sl_swin_s_epochs_25_batch_size_48_128.pkl\",\n",
    "        ),\n",
    "        \"rb\",\n",
    "    ) as pkl_file:\n",
    "        preds = _pickle.load(pkl_file)\n",
    "        pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro expression works better when `distance = k` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_average_precision.mean_average_precision import MeanAveragePrecision2d\n",
    "import numpy as np\n",
    "from spotting import *\n",
    "from evaluation import *\n",
    "\n",
    "reconstructed_clean_videos_ground_truth_labels_len = 0\n",
    "p = 0.55  # From our analysis, 0.55 achieved the highest F1-Score\n",
    "show_plot_or_not = True\n",
    "metric_fn = MeanAveragePrecision2d(num_classes=1)\n",
    "matrix = {\"precision\": [], \"recall\": [], \"F1-score\": [], \"expression_count\": 0}\n",
    "test_videos_preds = []\n",
    "\n",
    "for index, pred in enumerate(preds):\n",
    "    print(f\"test video {index} / {len(preds)} is in process.\")\n",
    "    test_video_preds = []\n",
    "    scores = np.array(pred)\n",
    "    aggregated_scores = scores.copy()\n",
    "\n",
    "    # Score aggregation\n",
    "    for x in range(len(scores[k:-k])):\n",
    "        aggregated_scores[x + k] = scores[x : x + 2 * k].mean()\n",
    "    aggregated_scores = aggregated_scores[k:-k]\n",
    "\n",
    "    # Moilanen threshold technique\n",
    "    threshold = aggregated_scores.mean() + p * (\n",
    "        max(aggregated_scores) - aggregated_scores.mean()\n",
    "    )\n",
    "    peaks, _ = find_peaks(aggregated_scores[:, 0], height=threshold[0], distance=k)\n",
    "\n",
    "    if len(peaks) != 0:\n",
    "        for peak in peaks:\n",
    "            # align the peak\n",
    "            peak = peak + k\n",
    "            # Extend left and right side of peak by k frames\n",
    "            test_video_preds.append([peak - k, peak + k])\n",
    "    else:\n",
    "        print(f\"test video {test_videos_name[index]} has no preds!\")\n",
    "    test_videos_preds.append(test_video_preds)\n",
    "\n",
    "    # Plot the result to see the peaks\n",
    "    # Note for some video the ground truth samples is below frame index 0\n",
    "    # due to the effect of aggregation, but no impact to the evaluation\n",
    "    if show_plot_or_not is True:\n",
    "        # frames = range(len(aggregated_scores))\n",
    "        plt.figure(figsize=(11, 4))\n",
    "        # plt.plot(frames, scores[k:-k], label=\"scores\")\n",
    "        # plt.plot(frames, aggregated_scores, label=\"aggregated scores\")\n",
    "        plt.plot(aggregated_scores, color=\"tab:blue\", label=\"aggregated scores\")\n",
    "        for peak in peaks:\n",
    "            plt.axvline(\n",
    "                # the peak is already aligned above\n",
    "                x=peak,\n",
    "                color=\"r\",\n",
    "            )\n",
    "        plt.axhline(y=threshold, color=\"g\", label=\"threshold\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    print(f\"The current test video be processed: {test_videos_name[index]}\")\n",
    "    print(f\"test video {index} / {len(preds)} is processed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print for csv submission\n",
    "print(\"vid,pred_onset,pred_offset,type\")\n",
    "type = \"me\" if expression_type == \"micro-expression\" else \"mae\"\n",
    "for test_video_name, test_video_preds in zip(test_videos_name, test_videos_preds):\n",
    "    if len(test_video_preds) != 0:\n",
    "        for test_video_pred in test_video_preds:\n",
    "            print(f\"{test_video_name},{test_video_pred[0]},{test_video_pred[1]},{type}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6696e3028ae55fa5be357812587f73779dfb157cc851dab91c4ca8ffd3c7a806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
